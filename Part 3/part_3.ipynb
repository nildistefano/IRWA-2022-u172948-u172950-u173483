{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO_DOs**:\n",
    "1. Repasar que el `bm25` funcione bien (mirar cómo se calcula pa cada doc e ir printeando los parámetros que usa y verificar que todo esté bien). Pd: el ranking ya funciona pero eso no quiere decir que no la haya liado en un parámetro o dos.\n",
    "2. Create \"our own score\" --> qué putas\n",
    "3. Cuando hagamos el print_top_20 pa comparar molaría tunear un poco el código pa que fuese aun más \"visualmente comparable\". Ahora mismo está simplemente en dos celdas separadas y es poco comparable.\n",
    "1000. Report-> explicar que hemos reutilizado código para el bm25 pero que necesitabamos cosas nuevas y que por eso hemos creado un diccionario docu_length y el lavg etc. Explicar que hemos usado k1=1.6 y b=0.75 que lo ponia en la teoria. Y que vemos claramente que tiene más en cuenta docu_length que el tf-idf y que para nosotros es mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------- #\n",
    "# SON LOS IMPORTS DEL LAB ANTERIOR, LOS DESCOMENTAMOS A MEDIDA QUE LOS VAYAMOS USANDO #\n",
    "# ----------------------------------------------------------------------------------- #\n",
    "\n",
    "# from collections import defaultdict\n",
    "from array import array\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "# import json\n",
    "# import random\n",
    "# import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "      <th>Clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>1575918182698979328</td>\n",
       "      <td>So this will keep spinning over us until 7 pm…...</td>\n",
       "      <td>suzjdean</td>\n",
       "      <td>Fri Sep 30 18:39:08 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/suzjdean/status/1575918182...</td>\n",
       "      <td>['keep', 'spin', 'us', 'away', 'alreadi', 'hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>1575918151862304768</td>\n",
       "      <td>Our hearts go out to all those affected by #Hu...</td>\n",
       "      <td>lytx</td>\n",
       "      <td>Fri Sep 30 18:39:01 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/lytx/status/15759181518623...</td>\n",
       "      <td>['heart', 'go', 'affect', 'hurricaneian', 'wis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document                   Id  \\\n",
       "0    doc_1  1575918182698979328   \n",
       "1    doc_2  1575918151862304768   \n",
       "\n",
       "                                                Text  Username  \\\n",
       "0  So this will keep spinning over us until 7 pm…...  suzjdean   \n",
       "1  Our hearts go out to all those affected by #Hu...      lytx   \n",
       "\n",
       "                             Date          Hashtags Mentions  Likes  Retweets  \\\n",
       "0  Fri Sep 30 18:39:08 +0000 2022  ['HurricaneIan']      NaN      0         0   \n",
       "1  Fri Sep 30 18:39:01 +0000 2022  ['HurricaneIan']      NaN      0         0   \n",
       "\n",
       "                                                 Url  \\\n",
       "0  https://twitter.com/suzjdean/status/1575918182...   \n",
       "1  https://twitter.com/lytx/status/15759181518623...   \n",
       "\n",
       "                                          Clean_text  \n",
       "0  ['keep', 'spin', 'us', 'away', 'alreadi', 'hur...  \n",
       "1  ['heart', 'go', 'affect', 'hurricaneian', 'wis...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder = '../output/'\n",
    "data_folder = '../data/'\n",
    "data = pd.read_csv(output_folder + \"lab1_tweets_df.csv\", sep='|')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document                                                  doc_1\n",
       "Id                                          1575918182698979328\n",
       "Text          So this will keep spinning over us until 7 pm…...\n",
       "Username                                               suzjdean\n",
       "Date                             Fri Sep 30 18:39:08 +0000 2022\n",
       "Hashtags                                       ['HurricaneIan']\n",
       "Mentions                                                    NaN\n",
       "Likes                                                         0\n",
       "Retweets                                                      0\n",
       "Url           https://twitter.com/suzjdean/status/1575918182...\n",
       "Clean_text    ['keep', 'spin', 'us', 'away', 'alreadi', 'hur...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Document', 'Id', 'Text', 'Username', 'Date', 'Hashtags', 'Mentions',\n",
       "       'Likes', 'Retweets', 'Url', 'Clean_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['scwx', 'HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>['CarrboroSafe', 'ncwx', 'HurricaneIan']</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>['Kissimmee', 'SaintCloud', 'BlueCounty', 'Dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>['HurricaneIan', 'Florida', 'MAGATears']</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>['DeSantis', 'HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Hashtags  Likes  Retweets\n",
       "0                                      ['HurricaneIan']      0         0\n",
       "1                                      ['HurricaneIan']      0         0\n",
       "2                                      ['HurricaneIan']      0         0\n",
       "3                              ['scwx', 'HurricaneIan']      0         0\n",
       "4                                      ['HurricaneIan']      0         0\n",
       "...                                                 ...    ...       ...\n",
       "3995           ['CarrboroSafe', 'ncwx', 'HurricaneIan']      2         0\n",
       "3996  ['Kissimmee', 'SaintCloud', 'BlueCounty', 'Dis...      0         0\n",
       "3997           ['HurricaneIan', 'Florida', 'MAGATears']     16         8\n",
       "3998                                   ['HurricaneIan']      2         1\n",
       "3999                       ['DeSantis', 'HurricaneIan']      0         0\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Hashtags', 'Likes', 'Retweets']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing and Ranking definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line=  line.lower() ## Transform in lowercase\n",
    "    line=  line.split() ## Tokenize the text to get a list of terms\n",
    "    line= [x for x in line if x not in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line= [stemmer.stem(x) for x in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(dataframe):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    dataframe -- DataFrame containing tweet information\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    num_documents = dataframe.shape[0]\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  # term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)   # document frequencies of terms in the corpus\n",
    "    idf = defaultdict(float)\n",
    "    for row in dataframe.iterrows():\n",
    "        doc_id = row[1]['Id']\n",
    "        terms = row[1]['Clean_text']  \n",
    "        terms = eval(terms)\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[doc_id, [position]]\n",
    "                \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        # calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] = df[term]+1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 167.27 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "new_index, tf, df, idf = create_index_tfidf(data)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keep'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(new_index.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index['pol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "def save_index_tfidf(index, tf, df, idf, path):\n",
    "\n",
    "    with open(join(path,'index.json'), 'w') as fp:\n",
    "        json.dump(index, fp)\n",
    "\n",
    "    with open(join(path,'tf.json'), 'w') as fp:\n",
    "        json.dump(tf, fp)\n",
    "\n",
    "    with open(join(path,'df.json'), 'w') as fp:\n",
    "        json.dump(df, fp)\n",
    "    \n",
    "    with open(join(path,'idf.json'), 'w') as fp:\n",
    "        json.dump(idf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index_tfidf(new_index, tf, df, idf, './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words analysis in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hurricaneian</td>\n",
       "      <td>3988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>florida</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>hurrican</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ian</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>help</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>amp</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>storm</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>carolina</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>flood</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>power</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>south</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>make</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>landfal</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>wind</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>damag</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>peopl</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>safe</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>get</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>impact</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>go</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word    DF\n",
       "5    hurricaneian  3988\n",
       "72        florida   881\n",
       "47       hurrican   793\n",
       "70            ian   781\n",
       "211          help   386\n",
       "122           amp   362\n",
       "25          storm   352\n",
       "87       carolina   297\n",
       "303         flood   289\n",
       "293         power   275\n",
       "86          south   260\n",
       "77           make   251\n",
       "78        landfal   245\n",
       "109          wind   238\n",
       "48          damag   231\n",
       "258         peopl   229\n",
       "15           safe   219\n",
       "388           get   216\n",
       "309        impact   214\n",
       "7              go   210"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ============================= ##\n",
    "## show top DF in the collection ##\n",
    "## ============================= ##\n",
    "\n",
    "word = []\n",
    "word_df = []\n",
    "for item in df.items():\n",
    "    word.append(item[0])\n",
    "    word_df.append(item[1])\n",
    "\n",
    "document_frequency_df = pd.DataFrame(list(zip(word,word_df)), columns=['Word', 'DF'])\n",
    "document_frequency_df.sort_values(by='DF', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based docuemnt frequency of the terms above, we define the following queries for our test cases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Floodings in South Carolina\", \n",
    "    \"HurracaineIan disaster\", \n",
    "    \"Damage of HuracaineIan\", \n",
    "    \"Florida floodings\", \n",
    "    \"Storm and wind in Florida\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New **function** `BestMatch25`:\n",
    "\n",
    "Formula: $RSV_d = \\sum_{t \\in q} log [\\frac{N}{dft}] * \\frac{(k+1)*tf_{td}}{k1*((1-b) + b*(\\frac{L_d}{L_{ave}}) ) + tfd} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will need `Ld` and `Lavg`, we will build a dictionnary with documents and their corresponding lenghts, and with it we will calculate Lave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_length = dict()\n",
    "for index, row in data.iterrows():\n",
    "    docu_length[row['Id']] = len(eval(row['Clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "with open(join('./','docu_length.json'), 'w') as fp:\n",
    "    json.dump(docu_length, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "lavg = 0\n",
    "for length in docu_length.values():\n",
    "    lavg += length\n",
    "lavg = round(lavg / len(docu_length))\n",
    "print(lavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8271"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_bm25(terms, docs, index, idf, tf, k1, b, N, docu_length, lavg):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if len(index[term])==0:\n",
    "            continue\n",
    "        \n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):         \n",
    "            if doc in docs:\n",
    "                dft = len(index[term]) #in how many documents does it appear\n",
    "                ld = docu_length[doc] #document length\n",
    "                doc_vectors[doc][termIndex] = np.log(N/dft) * (((k1+1)*tf[term][doc_index]) / (k1*((1-b)+b*( ld / lavg ) ) + tf[term][doc_index]))\n",
    "    \n",
    "    # Calculate the score of each doc (the sum of the results)    \n",
    "    doc_scores=[[np.sum(curDocVec), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(doc_scores) == 0:\n",
    "        print(\"No results found for query\")\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_dedicated(terms, docs, index, idf, tf, k1, b, N, docu_length, lavg, info):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    info -- Info that will be used to add puntuation to the document\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if len(index[term])==0:\n",
    "            continue\n",
    "        \n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):         \n",
    "            if doc in docs:\n",
    "                dft = len(index[term]) #in how many documents does it appear\n",
    "                ld = docu_length[doc] #document length\n",
    "                doc_vectors[doc][termIndex] = np.log(N/dft) * (((k1+1)*tf[term][doc_index]) / (k1*((1-b)+b*( ld / lavg ) ) + tf[term][doc_index]))\n",
    "    \n",
    "    # Calculate the score of each doc (the sum of the results)    \n",
    "    doc_scores=[[np.sum(curDocVec), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores = []\n",
    "    doc_scores_info = []\n",
    "\n",
    "    for doc, curDocVec in doc_vectors.items():\n",
    "        # BM score\n",
    "        bm_score = np.sum(curDocVec)\n",
    "        # Hashtag score\n",
    "        h_score = len([term for term in terms if term.lower() in [x.lower() for x in info[info['Document'] == doc]['Hashtags']]])/len(terms)\n",
    "        # Likes score\n",
    "        l_score = math.log(1+(info[info['Document'] == doc]['Likes']/1 + info['Likes'].max()))\n",
    "        # Retweets score\n",
    "        r_score = math.log(1+(info[info['Document'] == doc]['Retweets']/1 + info['Retweets'].max()))\n",
    "        # Relevance score\n",
    "        relevance_score = 0.4 * h_score + 0.3 * l_score + 0.3 * r_score\n",
    "        # Dedicated score\n",
    "        total_score = bm_score * (1+relevance_score)/relevance_score\n",
    "        doc_scores.append([total_score, doc])\n",
    "        doc_scores_info.append([bm_score, h_score, l_score, r_score, doc])\n",
    "\n",
    "\n",
    "    doc_scores.sort(reverse=True)\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(doc_scores) == 0:\n",
    "        print(\"No results found for query\")\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return doc_scores, doc_scores_info\n",
    "    \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's keep that as a reference\n",
    "def rank_documents(terms, docs, index, idf, tf):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm *idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n",
    "    \n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(doc_scores) == 0:\n",
    "        print(\"No results found for query\")\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index, ranking):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    if (ranking == \"bm25\"):\n",
    "        ranked_docs = rank_documents_bm25(query, docs, index, idf, tf, k1 = 1.6, b = 0.75, N = data.shape[0], docu_length = docu_length, lavg = lavg) #TO-DO new formula!! (new function, new attributes)\n",
    "    elif ranking == \"dedicated\":\n",
    "        ranked_docs, _ = rank_documents_dedicated(query, docs, index, idf, tf, k1 = 1.6, b = 0.75, N = data.shape[0], docu_length = docu_length, lavg = lavg, info=data[['Document', 'Hashtags', 'Likes', 'Retweets']])\n",
    "    elif (ranking == \"tf-idf_cosine-similarity\"):\n",
    "        ranked_docs = rank_documents(query, docs, index, idf, tf)\n",
    "    else:\n",
    "        print(\"We don't have this way of ranking... Maybe try with: \\n 1) bm25 \\n 2) dedicated-ranking (our own) \\n 3) classical-tf-idf\")\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Ranking results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Ranking for `TF-IDF + cosine similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== RANKING WITH TF-IDF + Cosine similarity ====================\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 605 for the searched query 'Floodings in South Carolina':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_254                                      \n",
      "\t- Score: 4.74475387696339\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian']\n",
      "=======================================2========================================\n",
      "                                    doc_1289                                     \n",
      "\t- Score: 4.315205653041319\n",
      "\t- Text:  ['flood', 'garden', 'citi', 'south', 'carolina', 'gardenc', 'southcarolina', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_2874                                     \n",
      "\t- Score: 4.10872348195652\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================4========================================\n",
      "                                    doc_2834                                     \n",
      "\t- Score: 4.10872348195652\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================5========================================\n",
      "                                    doc_174                                      \n",
      "\t- Score: 4.10872348195652\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian', 'go']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 131 for the searched query 'HurracaineIan disaster':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_3964                                     \n",
      "\t- Score: 4.132642169823945\n",
      "\t- Text:  ['duke', 'energi', 'donat', 'florida', 'disast', 'fund', 'manag', 'volunt', 'florida', 'foundat', 'assist', 'commun', 'affect', 'hurricaneian', 'contribut', 'florida', 'disast', 'fund', 'visit', 'text', 'disast', 'info']\n",
      "=======================================2========================================\n",
      "                                    doc_865                                      \n",
      "\t- Score: 3.792112455030452\n",
      "\t- Text:  ['hurricaneian', 'updat', 'orang', 'counti', 'ad', 'fema', 'disast', 'declar', 'survivor', 'appli', 'disast', 'assist', 'fema', 'app', 'info']\n",
      "=======================================3========================================\n",
      "                                    doc_3933                                     \n",
      "\t- Score: 3.6962351566905367\n",
      "\t- Text:  ['thank', 'fortmyer', 'hurricaneian', 'disast', 'floridaintheheart']\n",
      "=======================================4========================================\n",
      "                                    doc_259                                      \n",
      "\t- Score: 3.6962351566905367\n",
      "\t- Text:  ['iron', 'segment', 'philli', 'disast', 'foxnew', 'faulknerfocu', 'right', 'updat', 'hurricaneian', 'disast', 'new', 'new', 'homepag', 'capecor', 'impeachkrasn', 'prayforflorida']\n",
      "=======================================5========================================\n",
      "                                    doc_1635                                     \n",
      "\t- Score: 3.6962351566905367\n",
      "\t- Text:  ['know', 'disasterdistresshelplin', 'ddh', 'dedic', 'disast', 'crisi', 'counsel', 'us', 'resid', 'experienc', 'mental', 'health', 'concern', 'relat', 'natur', 'disast', 'hurricaneian', 'mentalhealthmatt']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 231 for the searched query 'Damage of HuracaineIan':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2687                                     \n",
      "\t- Score: 2.8749627271127562\n",
      "\t- Text:  ['blind', 'pass', 'bridg', 'sanibel', 'captiva', 'rd', 'signific', 'damag', 'structur', 'miss', 'damag', 'ian', 'hurricaneian', 'flwx']\n",
      "=======================================2========================================\n",
      "                                    doc_1555                                     \n",
      "\t- Score: 2.8749627271127562\n",
      "\t- Text:  ['damag', 'hurricaneian', 'catastroph', 'histor']\n",
      "=======================================3========================================\n",
      "                                    doc_1226                                     \n",
      "\t- Score: 2.638065798398665\n",
      "\t- Text:  ['friday', 'morn', 'damag', 'longer', 'hidden', 'water', 'locat', 'hurricaneian', 'damag', 'border', 'port', 'charlott', 'amp', 'flwx', 'peaceluvorganix', 'theweatherladi', 'kindnessforstev']\n",
      "=======================================4========================================\n",
      "                                    doc_3735                                     \n",
      "\t- Score: 2.5713666631296492\n",
      "\t- Text:  ['satellit', 'imageri', 'damag', 'hurricaneian', 'sanibel']\n",
      "=======================================5========================================\n",
      "                                    doc_372                                      \n",
      "\t- Score: 2.5713666631296492\n",
      "\t- Text:  ['survey', 'damag', 'hurricaneian', 'southwest', 'florida']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 1114 for the searched query 'Florida floodings':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_1317                                     \n",
      "\t- Score: 3.794380721457354\n",
      "\t- Text:  ['serfc', 'florida', 'brief', 'flood', 'hurricaneian', 'flwx', 'flood']\n",
      "=======================================2========================================\n",
      "                                    doc_1672                                     \n",
      "\t- Score: 3.250375687450309\n",
      "\t- Text:  ['edgewat', 'florida', 'flood', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_1493                                     \n",
      "\t- Score: 2.9694789323793103\n",
      "\t- Text:  ['wind', 'bad', 'us', 'lowcountri', 'like', 'florida', 'wind', 'worri', 'flood', 'lowcountri', 'flood', 'regular', 'rain', 'storm', 'hurrican', 'flood', 'mani', 'home', 'fast', 'hurricaneian']\n",
      "=======================================4========================================\n",
      "                                    doc_2901                                     \n",
      "\t- Score: 2.907136014855557\n",
      "\t- Text:  ['hurricaneian', 'florida', 'rain', 'flood', 'report']\n",
      "=======================================5========================================\n",
      "                                    doc_824                                      \n",
      "\t- Score: 2.8189040144223148\n",
      "\t- Text:  ['decis', 'support', 'brief', 'address', 'potenti', 'flood', 'carolina', 'virginia', 'hurricaneian', 'flood']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 1333 for the searched query 'Storm and wind in Florida':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_370                                      \n",
      "\t- Score: 2.8184075410327574\n",
      "\t- Text:  ['hurricaneian', 'advisori', 'max', 'wind', 'sshw', 'mslp', 'center', 'ian', 'make', 'landfal', 'storm', 'surg', 'damag', 'wind', 'flash', 'flood', 'lash', 'carolina']\n",
      "=======================================2========================================\n",
      "                                    doc_2794                                     \n",
      "\t- Score: 2.77975276449882\n",
      "\t- Text:  ['storm', 'chaser', 'reed', 'timmer', 'reedtimmeraccu', 'record', 'chaotic', 'scene', 'flood', 'storm', 'surg', 'howl', 'wind', 'pine', 'island', 'florida', 'enter', 'eye', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_1417                                     \n",
      "\t- Score: 2.7583100467507595\n",
      "\t- Text:  ['wind', 'resili', 'capecor', 'hurricaneian', 'better', 'ever', 'would', 'expect', 'pic', 'show', 'home', 'block', 'obviou', 'wind', 'damag', 'despit', 'cat', 'wind', 'gust']\n",
      "=======================================4========================================\n",
      "                                    doc_3501                                     \n",
      "\t- Score: 2.750342242635723\n",
      "\t- Text:  ['hurricaneian', 'advisori', 'max', 'wind', 'sshw', 'mslp', 'hurrican', 'ian', 'acceler', 'toward', 'south', 'carolina', 'coast', 'storm', 'surg', 'damag', 'wind', 'arriv', 'soon']\n",
      "=======================================5========================================\n",
      "                                    doc_557                                      \n",
      "\t- Score: 2.746573006641334\n",
      "\t- Text:  ['pm', 'updat', 'hurricaneian', 'make', 'landfal', 'center', 'storm', 'mile', 'ene', 'charleston', 'sc', 'mph', 'wind', 'still', 'cat', 'hurrican', 'flash', 'flood', 'damag', 'wind', 'life', 'threaten', 'storm', 'surg', 'expect', 'weaken', 'rapidli', 'continu', 'move', 'land']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{' RANKING WITH TF-IDF + Cosine similarity ':=^80}\")\n",
    "for query in queries:\n",
    "    ranked_docs = search_tf_idf(query, new_index, ranking =\"tf-idf_cosine-similarity\")\n",
    "    top = 5\n",
    "    print(f\"{'='}\" * 80)\n",
    "    print(f\"\\nTop {top} results out of {len(ranked_docs)} for the searched query '{query}':\\n\")\n",
    "    for rank, doc in enumerate(ranked_docs[:top], start=1):\n",
    "        print(f\"{rank:=^80}\")\n",
    "        print(f\"{doc[1]:^80} \\n\\t- Score: {doc[0]}\\n\\t- Text:  {data[data['Document']==doc[1]]['Clean_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Ranking for `our own score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our own score method will make use of the extra information we have about the tweets. The formula will take the BM25 score as a base and potentiate it based on `Hashtags`, `Likes` and `Retweets`.\n",
    "\n",
    "Formula: $\\text{Dedicated score} = \\text{BM25score} · \\frac{1+\\text{Rscore}}{\\text{Rscore}} $\n",
    "\n",
    "Where\n",
    "$\\text{Rsocre} = 0.4·\\frac{\\text{N hashtags appear in the query}}{\\text{query length}} + 0.3 · log(\\frac{\\text{N Likes}}{max(\\text{N likes})}) + 0.3 · log(\\frac{\\text{N Retweets}}{max(\\text{N Retweets})})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= RANKING WITH Dedicated-score =========================\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 605 for the searched query 'Floodings in South Carolina':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_254                                      \n",
      "\t- Score: 8.252756763942823\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian']\n",
      "=======================================2========================================\n",
      "                                    doc_2874                                     \n",
      "\t- Score: 7.131251231086775\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_2834                                     \n",
      "\t- Score: 7.131251231086775\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================4========================================\n",
      "                                    doc_174                                      \n",
      "\t- Score: 7.131167886485727\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian', 'go']\n",
      "=======================================5========================================\n",
      "                                    doc_1289                                     \n",
      "\t- Score: 6.589942910211281\n",
      "\t- Text:  ['flood', 'garden', 'citi', 'south', 'carolina', 'gardenc', 'southcarolina', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 131 for the searched query 'HurracaineIan disaster':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_3933                                     \n",
      "\t- Score: 3.9991630977780805\n",
      "\t- Text:  ['thank', 'fortmyer', 'hurricaneian', 'disast', 'floridaintheheart']\n",
      "=======================================2========================================\n",
      "                                    doc_800                                      \n",
      "\t- Score: 3.1509325747809567\n",
      "\t- Text:  ['hurricaneian', 'live', 'chopper', 'feed', 'florida', 'disast', 'area']\n",
      "=======================================3========================================\n",
      "                                    doc_2315                                     \n",
      "\t- Score: 3.1505172321427612\n",
      "\t- Text:  ['hurricaneian', 'disast', 'relief', 'help', 'greatli', 'appreci', 'learn']\n",
      "=======================================4========================================\n",
      "                                    doc_1408                                     \n",
      "\t- Score: 2.874382336733691\n",
      "\t- Text:  ['flumc', 'disast', 'recoveri', 'hotlin', 'open', 'flumc', 'hurricaneian']\n",
      "=======================================5========================================\n",
      "                                    doc_2201                                     \n",
      "\t- Score: 2.829942049513056\n",
      "\t- Text:  ['pregnant', 'cdcgov', 'outlin', 'help', 'guidanc', 'follow', 'disast', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 231 for the searched query 'Damage of HuracaineIan':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_1555                                     \n",
      "\t- Score: 3.812117129888802\n",
      "\t- Text:  ['damag', 'hurricaneian', 'catastroph', 'histor']\n",
      "=======================================2========================================\n",
      "                                    doc_3068                                     \n",
      "\t- Score: 3.335702571669936\n",
      "\t- Text:  ['well', 'check', 'hurricaneian', 'venicefl', 'damag']\n",
      "=======================================3========================================\n",
      "                                    doc_3735                                     \n",
      "\t- Score: 3.335663586533802\n",
      "\t- Text:  ['satellit', 'imageri', 'damag', 'hurricaneian', 'sanibel']\n",
      "=======================================4========================================\n",
      "                                    doc_3658                                     \n",
      "\t- Score: 3.335663586533802\n",
      "\t- Text:  ['get', 'plantat', 'venicefl', 'hurricaneian', 'damag']\n",
      "=======================================5========================================\n",
      "                                    doc_3060                                     \n",
      "\t- Score: 3.335624635225991\n",
      "\t- Text:  ['wait', 'trump', 'say', 'sever', 'damag']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 1114 for the searched query 'Florida floodings':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_1672                                     \n",
      "\t- Score: 5.535832009686089\n",
      "\t- Text:  ['edgewat', 'florida', 'flood', 'hurricaneian']\n",
      "=======================================2========================================\n",
      "                                    doc_2901                                     \n",
      "\t- Score: 4.843493245964907\n",
      "\t- Text:  ['hurricaneian', 'florida', 'rain', 'flood', 'report']\n",
      "=======================================3========================================\n",
      "                                    doc_1317                                     \n",
      "\t- Score: 4.785331749618678\n",
      "\t- Text:  ['serfc', 'florida', 'brief', 'flood', 'hurricaneian', 'flwx', 'flood']\n",
      "=======================================4========================================\n",
      "                                    doc_3998                                     \n",
      "\t- Score: 4.278145847255807\n",
      "\t- Text:  ['realli', 'hurricaneian', 'flood', 'florida', 'magatear', 'flute']\n",
      "=======================================5========================================\n",
      "                                    doc_1147                                     \n",
      "\t- Score: 4.065520479224452\n",
      "\t- Text:  ['flood', 'portsmouth', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 1333 for the searched query 'Storm and wind in Florida':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2950                                     \n",
      "\t- Score: 5.070607660682245\n",
      "\t- Text:  ['hurricaneian', 'florida', 'signific', 'wind', 'gust']\n",
      "=======================================2========================================\n",
      "                                    doc_772                                      \n",
      "\t- Score: 4.36610958899164\n",
      "\t- Text:  ['wind', 'wild', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_2813                                     \n",
      "\t- Score: 3.929608108477545\n",
      "\t- Text:  ['edt', 'nation', 'hurrican', 'center', 'hurricaneian', 'storm', 'forc', 'wind', 'probabl']\n",
      "=======================================4========================================\n",
      "                                    doc_2934                                     \n",
      "\t- Score: 3.929181509341467\n",
      "\t- Text:  ['hurricaneian', 'tropic', 'storm', 'wind', 'extend', 'mile', 'look', 'compar', 'texa']\n",
      "=======================================5========================================\n",
      "                                    doc_3012                                     \n",
      "\t- Score: 3.928983126888457\n",
      "\t- Text:  ['advisori', 'storm', 'surg', 'damag', 'wind', 'arriv', 'soon', 'carolina', 'hurricaneian']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{' RANKING WITH Dedicated-score ':=^80}\")\n",
    "for query in queries:\n",
    "    ranked_docs = search_tf_idf(query, new_index, ranking =\"dedicated\")\n",
    "    top = 5\n",
    "    print(f\"{'='}\" * 80)\n",
    "    print(f\"\\nTop {top} results out of {len(ranked_docs)} for the searched query '{query}':\\n\")\n",
    "    for rank, doc in enumerate(ranked_docs[:top], start=1):\n",
    "        print(f\"{rank:=^80}\")\n",
    "        print(f\"{doc[1]:^80} \\n\\t- Score: {doc[0]}\\n\\t- Text:  {data[data['Document']==doc[1]]['Clean_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Ranking for `BM25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_docs = search_tf_idf(query, new_index, ranking =\"bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== RANKING WITH BM25 ===============================\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 605 for the searched query 'Floodings in South Carolina':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_254                                      \n",
      "\t- Score: 6.577248263193381\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian']\n",
      "=======================================2========================================\n",
      "                                    doc_2874                                     \n",
      "\t- Score: 5.683435379919324\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_2834                                     \n",
      "\t- Score: 5.683435379919324\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================4========================================\n",
      "                                    doc_174                                      \n",
      "\t- Score: 5.683435379919324\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian', 'go']\n",
      "=======================================5========================================\n",
      "                                    doc_1289                                     \n",
      "\t- Score: 5.25208707517901\n",
      "\t- Text:  ['flood', 'garden', 'citi', 'south', 'carolina', 'gardenc', 'southcarolina', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 131 for the searched query 'HurracaineIan disaster':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_3933                                     \n",
      "\t- Score: 3.187273866185845\n",
      "\t- Text:  ['thank', 'fortmyer', 'hurricaneian', 'disast', 'floridaintheheart']\n",
      "=======================================2========================================\n",
      "                                    doc_800                                      \n",
      "\t- Score: 2.5112466794097013\n",
      "\t- Text:  ['hurricaneian', 'live', 'chopper', 'feed', 'florida', 'disast', 'area']\n",
      "=======================================3========================================\n",
      "                                    doc_2315                                     \n",
      "\t- Score: 2.5112466794097013\n",
      "\t- Text:  ['hurricaneian', 'disast', 'relief', 'help', 'greatli', 'appreci', 'learn']\n",
      "=======================================4========================================\n",
      "                                    doc_1408                                     \n",
      "\t- Score: 2.2908134545580774\n",
      "\t- Text:  ['flumc', 'disast', 'recoveri', 'hotlin', 'open', 'flumc', 'hurricaneian']\n",
      "=======================================5========================================\n",
      "                                    doc_2425                                     \n",
      "\t- Score: 2.25542197622416\n",
      "\t- Text:  ['someth', 'see', 'disast', 'movi', 'real', 'devast', 'fortmyer', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 231 for the searched query 'Damage of HuracaineIan':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_1555                                     \n",
      "\t- Score: 3.038624187257626\n",
      "\t- Text:  ['damag', 'hurricaneian', 'catastroph', 'histor']\n",
      "=======================================2========================================\n",
      "                                    doc_3735                                     \n",
      "\t- Score: 2.6584745647542727\n",
      "\t- Text:  ['satellit', 'imageri', 'damag', 'hurricaneian', 'sanibel']\n",
      "=======================================3========================================\n",
      "                                    doc_372                                      \n",
      "\t- Score: 2.6584745647542727\n",
      "\t- Text:  ['survey', 'damag', 'hurricaneian', 'southwest', 'florida']\n",
      "=======================================4========================================\n",
      "                                    doc_3658                                     \n",
      "\t- Score: 2.6584745647542727\n",
      "\t- Text:  ['get', 'plantat', 'venicefl', 'hurricaneian', 'damag']\n",
      "=======================================5========================================\n",
      "                                    doc_3409                                     \n",
      "\t- Score: 2.6584745647542727\n",
      "\t- Text:  ['well', 'check', 'hurricaneian', 'damag', 'venicefl']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 1114 for the searched query 'Florida floodings':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_1672                                     \n",
      "\t- Score: 4.412130701641046\n",
      "\t- Text:  ['edgewat', 'florida', 'flood', 'hurricaneian']\n",
      "=======================================2========================================\n",
      "                                    doc_2901                                     \n",
      "\t- Score: 3.860147396927724\n",
      "\t- Text:  ['hurricaneian', 'florida', 'rain', 'flood', 'report']\n",
      "=======================================3========================================\n",
      "                                    doc_1317                                     \n",
      "\t- Score: 3.813794085934804\n",
      "\t- Text:  ['serfc', 'florida', 'brief', 'flood', 'hurricaneian', 'flwx', 'flood']\n",
      "=======================================4========================================\n",
      "                                    doc_3998                                     \n",
      "\t- Score: 3.411362735506983\n",
      "\t- Text:  ['realli', 'hurricaneian', 'flood', 'florida', 'magatear', 'flute']\n",
      "=======================================5========================================\n",
      "                                    doc_1147                                     \n",
      "\t- Score: 3.2402601285073604\n",
      "\t- Text:  ['flood', 'portsmouth', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 5 results out of 1333 for the searched query 'Storm and wind in Florida':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2950                                     \n",
      "\t- Score: 4.041152112378967\n",
      "\t- Text:  ['hurricaneian', 'florida', 'signific', 'wind', 'gust']\n",
      "=======================================2========================================\n",
      "                                    doc_772                                      \n",
      "\t- Score: 3.4796841264696923\n",
      "\t- Text:  ['wind', 'wild', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_3012                                     \n",
      "\t- Score: 3.1318029654573767\n",
      "\t- Text:  ['advisori', 'storm', 'surg', 'damag', 'wind', 'arriv', 'soon', 'carolina', 'hurricaneian']\n",
      "=======================================4========================================\n",
      "                                    doc_2934                                     \n",
      "\t- Score: 3.1318029654573767\n",
      "\t- Text:  ['hurricaneian', 'tropic', 'storm', 'wind', 'extend', 'mile', 'look', 'compar', 'texa']\n",
      "=======================================5========================================\n",
      "                                    doc_2813                                     \n",
      "\t- Score: 3.1318029654573767\n",
      "\t- Text:  ['edt', 'nation', 'hurrican', 'center', 'hurricaneian', 'storm', 'forc', 'wind', 'probabl']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{' RANKING WITH BM25 ':=^80}\")\n",
    "for query in queries:\n",
    "    ranked_docs = search_tf_idf(query, new_index, ranking =\"bm25\")\n",
    "    top = 5\n",
    "    print(f\"{'='}\" * 80)\n",
    "    print(f\"\\nTop {top} results out of {len(ranked_docs)} for the searched query '{query}':\\n\")\n",
    "    for rank, doc in enumerate(ranked_docs[:top], start=1):\n",
    "        print(f\"{rank:=^80}\")\n",
    "        print(f\"{doc[1]:^80} \\n\\t- Score: {doc[0]}\\n\\t- Text:  {data[data['Document']==doc[1]]['Clean_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Word2Vec with cosine similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will rank our documents using cosine similarity. The thing is that instead of TF-IDF vector representation of the queries we will use word2vec.\n",
    "\n",
    "First we train the `model` using our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mast\u001b[39;00m \u001b[39mimport\u001b[39;00m literal_eval\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      3\u001b[0m words \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m _, tweet \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39miterrows():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from gensim.models import Word2Vec\n",
    "words = []\n",
    "for _, tweet in data.iterrows():\n",
    "    words.append(literal_eval(tweet['Clean_text']))\n",
    "model = Word2Vec(words, min_count = 1, vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a `tweet2vec` representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet2vec_builder(model):\n",
    "    \"\"\"\n",
    "    Build a tweet2vec dictionary containing the average of the word2vec vectors for each document.\n",
    "\n",
    "    Argument:\n",
    "    model -- pretrained model \n",
    "\n",
    "    Returns:\n",
    "    dictionary with the vector representation of each doc\n",
    "    \"\"\"\n",
    "    tweet2vec = {}\n",
    "    for _, tweet in data.iterrows():\n",
    "        doc_vec = [model.wv[term] for term in literal_eval(tweet['Clean_text'])]\n",
    "        tweet2vec[tweet['Document']] = np.mean(doc_vec, axis=0)\n",
    "    return tweet2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2vec = tweet2vec_builder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the cosine similarity between pairs of vector representations for **queries** and **tweets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_cosine_similarity(query, docs, tweet2vec, model):\n",
    "    \"\"\"\n",
    "    Rank documents based on the cosine similarity with the query.\n",
    "    \n",
    "    Argument:\n",
    "    query -- list of terms in the query\n",
    "    docs -- documents containing the query terms\n",
    "    tweet2vec -- dictionary containing the tweet2vec representation of each document\n",
    "    model -- pretrained word2vec model\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # Query word2vec matrix. Contains len(terms) word2vec vectors.\n",
    "    query_vec = np.array([model.wv[term] for term in query if term in model.wv.key_to_index])\n",
    "    # Averaging all the word2vec of the query to optain query2vec\n",
    "    query_vec = np.mean(query_vec, axis=0)\n",
    "\n",
    "    # Cosine similarity between query2vec and tweet2vec (which have the same dimension)\n",
    "    doc_scores=[[np.dot(doc_vec, query_vec), doc] for doc, doc_vec in tweet2vec.items() if doc in docs]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    if len(doc_scores) == 0:\n",
    "        print(\"No results found for query\")\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word2vec(query, tweet2vec, model, index):\n",
    "    \"\"\"\n",
    "    Preprocess the query, find the docs containing the query terms and executing the ranking.\n",
    "    \n",
    "    Argument:\n",
    "    query -- string\n",
    "    tweet2vec -- dictionary containing the tweet2vec representation of each document\n",
    "    model -- pretrained word2vec model\n",
    "    index -- inverted index data structure\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "    \n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "\n",
    "    # We will make sure that the returned docs contain at least one of the terms in the query.\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            # Term is not in any doc.\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = word2vec_cosine_similarity(query, docs, tweet2vec, model)\n",
    "    return ranked_docs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ranking** for `Word2Vec + Cosine similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== RANKING WITH Word2Vec + Cosine similarity ===================\n",
      "================================================================================\n",
      "\n",
      "Top 20 results out of 605 for the searched query 'Floodings in South Carolina':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2901                                     \n",
      "\t- Score: 28.15808868408203\n",
      "\t- Text:  ['hurricaneian', 'florida', 'rain', 'flood', 'report']\n",
      "=======================================2========================================\n",
      "                                    doc_174                                      \n",
      "\t- Score: 26.271076202392578\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian', 'go']\n",
      "=======================================3========================================\n",
      "                                    doc_254                                      \n",
      "\t- Score: 26.261526107788086\n",
      "\t- Text:  ['south', 'carolina', 'hurricaneian']\n",
      "=======================================4========================================\n",
      "                                    doc_2877                                     \n",
      "\t- Score: 25.7681827545166\n",
      "\t- Text:  ['hurrican', 'ian', 'cuba', 'florida', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================5========================================\n",
      "                                    doc_2874                                     \n",
      "\t- Score: 25.601783752441406\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================6========================================\n",
      "                                    doc_2834                                     \n",
      "\t- Score: 25.601783752441406\n",
      "\t- Text:  ['charleston', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================7========================================\n",
      "                                    doc_2505                                     \n",
      "\t- Score: 24.242753982543945\n",
      "\t- Text:  ['hurricaneian', 'make', 'landfal', 'south', 'carolina']\n",
      "=======================================8========================================\n",
      "                                    doc_249                                      \n",
      "\t- Score: 24.242753982543945\n",
      "\t- Text:  ['hurricaneian', 'make', 'landfal', 'south', 'carolina']\n",
      "=======================================9========================================\n",
      "                                    doc_1672                                     \n",
      "\t- Score: 24.17037582397461\n",
      "\t- Text:  ['edgewat', 'florida', 'flood', 'hurricaneian']\n",
      "=======================================10=======================================\n",
      "                                    doc_2545                                     \n",
      "\t- Score: 24.102266311645508\n",
      "\t- Text:  ['busi', 'open', 'south', 'florida', 'hurricaneian']\n",
      "=======================================11=======================================\n",
      "                                    doc_823                                      \n",
      "\t- Score: 23.82710075378418\n",
      "\t- Text:  ['storm', 'surg', 'flood', 'myrtl', 'beach', 'hurricaneian']\n",
      "=======================================12=======================================\n",
      "                                    doc_3503                                     \n",
      "\t- Score: 23.546064376831055\n",
      "\t- Text:  ['hurricaneian', 'make', 'way', 'carolina', 'florida', 'work', 'survey', 'damag']\n",
      "=======================================13=======================================\n",
      "                                    doc_345                                      \n",
      "\t- Score: 23.366710662841797\n",
      "\t- Text:  ['new', 'hurricaneian', 'made', 'landfal', 'time', 'south', 'carolina']\n",
      "=======================================14=======================================\n",
      "                                    doc_2635                                     \n",
      "\t- Score: 23.207332611083984\n",
      "\t- Text:  ['live', 'updat', 'hurricaneian', 'near', 'landfal', 'south', 'carolina']\n",
      "=======================================15=======================================\n",
      "                                    doc_3372                                     \n",
      "\t- Score: 23.15803337097168\n",
      "\t- Text:  ['safe', 'south', 'carolina', 'around', 'time', 'prayer', 'hurricaneian']\n",
      "=======================================16=======================================\n",
      "                                    doc_2204                                     \n",
      "\t- Score: 23.091026306152344\n",
      "\t- Text:  ['get', 'latest', 'hurricaneian', 'near', 'south', 'carolina']\n",
      "=======================================17=======================================\n",
      "                                    doc_2918                                     \n",
      "\t- Score: 23.063457489013672\n",
      "\t- Text:  ['hurrican', 'ian', 'begin', 'make', 'landfal', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================18=======================================\n",
      "                                    doc_636                                      \n",
      "\t- Score: 22.69416618347168\n",
      "\t- Text:  ['hurricaneian', 'make', 'landfal', 'south', 'carolina', 'categori', 'hurrican']\n",
      "=======================================19=======================================\n",
      "                                    doc_919                                      \n",
      "\t- Score: 22.657058715820312\n",
      "\t- Text:  ['live', 'continu', 'follow', 'hurrican', 'ian', 'impact', 'around', 'charleston', 'south', 'carolina', 'chswx', 'scwx', 'hurricaneian']\n",
      "=======================================20=======================================\n",
      "                                    doc_152                                      \n",
      "\t- Score: 22.65640640258789\n",
      "\t- Text:  ['pray', 'peopl', 'one', 'mani', 'place', 'call', 'home', 'south', 'carolina', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 20 results out of 131 for the searched query 'HurracaineIan disaster':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2072                                     \n",
      "\t- Score: 26.739181518554688\n",
      "\t- Text:  ['donat', 'florida', 'disast', 'fund', 'volunt', 'florida', 'help', 'floridian', 'recov', 'hurricaneian']\n",
      "=======================================2========================================\n",
      "                                    doc_1064                                     \n",
      "\t- Score: 25.67450714111328\n",
      "\t- Text:  ['look', 'support', 'affect', 'hurricaneian', 'feedingamerica', 'hurrican', 'ian', 'disast', 'relief', 'fund']\n",
      "=======================================3========================================\n",
      "                                    doc_800                                      \n",
      "\t- Score: 25.20431900024414\n",
      "\t- Text:  ['hurricaneian', 'live', 'chopper', 'feed', 'florida', 'disast', 'area']\n",
      "=======================================4========================================\n",
      "                                    doc_268                                      \n",
      "\t- Score: 24.91534423828125\n",
      "\t- Text:  ['heart', 'affect', 'advers', 'hurricaneian', 'florida', 'disast', 'fund', 'work', 'provid', 'help', 'relief', 'donat']\n",
      "=======================================5========================================\n",
      "                                    doc_758                                      \n",
      "\t- Score: 24.662586212158203\n",
      "\t- Text:  ['incred', 'disast', 'florida', 'hurricaneian', 'watch', 'near', 'carolina', 'send', 'prayer', 'affect', 'time']\n",
      "=======================================6========================================\n",
      "                                    doc_3933                                     \n",
      "\t- Score: 24.05408477783203\n",
      "\t- Text:  ['thank', 'fortmyer', 'hurricaneian', 'disast', 'floridaintheheart']\n",
      "=======================================7========================================\n",
      "                                    doc_1199                                     \n",
      "\t- Score: 22.974987030029297\n",
      "\t- Text:  ['right', 'work', 'start', 'get', 'west', 'coast', 'florida', 'back', 'feet', 'back', 'onlin', 'disast', 'relief', 'amp', 'crew', 'work', 'way', 'toward', 'myer', 'hurricaneian', 'atlnewsfirst']\n",
      "=======================================8========================================\n",
      "                                    doc_1252                                     \n",
      "\t- Score: 22.790142059326172\n",
      "\t- Text:  ['text', 'disast', 'visit', 'page', 'donat', 'florida', 'disast', 'fund', 'mani', 'thank', 'coastercreatur', 'share', 'florida', 'hurricaneian', 'donat']\n",
      "=======================================9========================================\n",
      "                                    doc_759                                      \n",
      "\t- Score: 22.71124267578125\n",
      "\t- Text:  ['florida', 'strong', 'keep', 'shine', 'look', 'way', 'help', 'floridian', 'affect', 'hurrican', 'ian', 'pleas', 'visit', 'text', 'disast', 'thepalmbeach', 'hurricaneian', 'worth', 'beach', 'florida']\n",
      "=======================================10=======================================\n",
      "                                    doc_3560                                     \n",
      "\t- Score: 22.343244552612305\n",
      "\t- Text:  ['join', 'ml', 'redcross', 'help', 'peopl', 'affect', 'hurricaneian', 'donat', 'help', 'emerg', 'prepared', 'respons', 'recoveri', 'disast']\n",
      "=======================================11=======================================\n",
      "                                    doc_3084                                     \n",
      "\t- Score: 22.302297592163086\n",
      "\t- Text:  ['mention', 'show', 'make', 'donat', 'redcross', 'support', 'disast', 'relief', 'hurricaneian', 'florida', 'typhoonmerbok', 'alaska', 'also', 'like', 'donat']\n",
      "=======================================12=======================================\n",
      "                                    doc_1426                                     \n",
      "\t- Score: 22.143901824951172\n",
      "\t- Text:  ['florida', 'prepar', 'assist', 'client', 'impact', 'hurrican', 'ian', 'whether', 'requir', 'restor', 'servic', 'need', 'gener', 'prepar', 'help', 'contact', 'disast', 'power', 'gener', 'hurricaneian']\n",
      "=======================================13=======================================\n",
      "                                    doc_2315                                     \n",
      "\t- Score: 22.078401565551758\n",
      "\t- Text:  ['hurricaneian', 'disast', 'relief', 'help', 'greatli', 'appreci', 'learn']\n",
      "=======================================14=======================================\n",
      "                                    doc_2474                                     \n",
      "\t- Score: 21.979820251464844\n",
      "\t- Text:  ['still', 'earli', 'grasp', 'full', 'impact', 'hurricaneian', 'florida', 'know', 'loss', 'great', 'would', 'like', 'assist', 'impact', 'consid', 'donat', 'florida', 'disast', 'fund', 'text', 'disast']\n",
      "=======================================15=======================================\n",
      "                                    doc_475                                      \n",
      "\t- Score: 21.935094833374023\n",
      "\t- Text:  ['disast', 'partner', 'redcross', 'ground', 'help', 'commun', 'need', 'aftermath', 'hurricaneian', 'join', 'fox', 'support', 'relief', 'effort', 'today']\n",
      "=======================================16=======================================\n",
      "                                    doc_3208                                     \n",
      "\t- Score: 21.935094833374023\n",
      "\t- Text:  ['disast', 'partner', 'redcross', 'ground', 'help', 'commun', 'need', 'aftermath', 'hurricaneian', 'join', 'fox', 'support', 'relief', 'effort', 'today']\n",
      "=======================================17=======================================\n",
      "                                    doc_2261                                     \n",
      "\t- Score: 21.912410736083984\n",
      "\t- Text:  ['govrondesanti', 'said', 'thousand', 'florida', 'resid', 'need', 'help', 'rebuild', 'hurricaneian', 'urg', 'could', 'contribut', 'florida', 'disast', 'fund']\n",
      "=======================================18=======================================\n",
      "                                    doc_509                                      \n",
      "\t- Score: 21.647756576538086\n",
      "\t- Text:  ['miamifound', 'rais', 'money', 'hurrican', 'relief', 'give', 'today', 'help', 'famili', 'live', 'uproot', 'natur', 'disast', 'hurricaneian', 'florida', 'floridahurrican']\n",
      "=======================================19=======================================\n",
      "                                    doc_3666                                     \n",
      "\t- Score: 21.579553604125977\n",
      "\t- Text:  ['help', 'peopl', 'affect', 'hurricaneian', 'donat', 'enabl', 'red', 'cross', 'prepar', 'respond', 'help', 'peopl', 'recov', 'disast', 'visit', 'call', 'cross', 'text', 'word', 'ian', 'make', 'donat']\n",
      "=======================================20=======================================\n",
      "                                    doc_2995                                     \n",
      "\t- Score: 21.128780364990234\n",
      "\t- Text:  ['hurricaneian', 'updat', 'versail', 'disast', 'water', 'flood', 'hous', 'even', 'wrap', 'head', 'around', 'damag', 'luckili', 'everybodi', 'pleas', 'stay', 'safe']\n",
      "================================================================================\n",
      "\n",
      "Top 20 results out of 231 for the searched query 'Damage of HuracaineIan':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_3503                                     \n",
      "\t- Score: 27.80498504638672\n",
      "\t- Text:  ['hurricaneian', 'make', 'way', 'carolina', 'florida', 'work', 'survey', 'damag']\n",
      "=======================================2========================================\n",
      "                                    doc_372                                      \n",
      "\t- Score: 24.99457359313965\n",
      "\t- Text:  ['survey', 'damag', 'hurricaneian', 'southwest', 'florida']\n",
      "=======================================3========================================\n",
      "                                    doc_3340                                     \n",
      "\t- Score: 24.726829528808594\n",
      "\t- Text:  ['good', 'morn', 'check', 'amp', 'hope', 'surviv', 'hurricaneian', 'without', 'damag']\n",
      "=======================================4========================================\n",
      "                                    doc_2047                                     \n",
      "\t- Score: 24.122217178344727\n",
      "\t- Text:  ['hurricaneian', 'updat', 'catastroph', 'aftermath', 'florida', 'amp', 'landfal', 'southcarolina', 'hurrican', 'ian', 'rescu', 'florida', 'flood', 'damag', 'wind', 'severeweath', 'weather', 'tropicalstorm', 'forecast', 'via', 'youtub']\n",
      "=======================================5========================================\n",
      "                                    doc_353                                      \n",
      "\t- Score: 23.67411231994629\n",
      "\t- Text:  ['miami', 'ap', 'hurricaneian', 'make', 'landfal', 'time', 'south', 'carolina', 'coast', 'caus', 'catastroph', 'damag', 'florida']\n",
      "=======================================6========================================\n",
      "                                    doc_240                                      \n",
      "\t- Score: 23.67411231994629\n",
      "\t- Text:  ['miami', 'ap', 'hurricaneian', 'make', 'landfal', 'time', 'south', 'carolina', 'coast', 'caus', 'catastroph', 'damag', 'florida']\n",
      "=======================================7========================================\n",
      "                                    doc_2129                                     \n",
      "\t- Score: 23.384418487548828\n",
      "\t- Text:  ['find', 'photo', 'imag', 'damag', 'today', 'mb', 'area', 'hurricaneian']\n",
      "=======================================8========================================\n",
      "                                    doc_722                                      \n",
      "\t- Score: 23.249799728393555\n",
      "\t- Text:  ['hurricaneian', 'travel', 'throughout', 'state', 'florida', 'set', 'make', 'landfal', 'charleston', 'myrtl', 'beach', 'south', 'carolina', 'afternoon', 'restor', 'help', 'time', 'crisi', 'storm', 'damag', 'assist', 'call', 'us']\n",
      "=======================================9========================================\n",
      "                                    doc_1889                                     \n",
      "\t- Score: 23.14735984802246\n",
      "\t- Text:  ['terribl', 'damag', 'orlando', 'take', 'care', 'peopl', 'hurricaneian']\n",
      "=======================================10=======================================\n",
      "                                    doc_3140                                     \n",
      "\t- Score: 23.128398895263672\n",
      "\t- Text:  ['coast', 'robertraywx', 'give', 'look', 'devast', 'hurricaneian', 'damag', 'fort', 'myer', 'florida', 'say', 'peopl', 'begin', 'clean']\n",
      "=======================================11=======================================\n",
      "                                    doc_1998                                     \n",
      "\t- Score: 23.068069458007812\n",
      "\t- Text:  ['thank', 'safe', 'well', 'alway', 'appreci', 'cover', 'damag', 'recoveri', 'fort', 'myer', 'today', 'hurricaneian']\n",
      "=======================================12=======================================\n",
      "                                    doc_3012                                     \n",
      "\t- Score: 23.02150535583496\n",
      "\t- Text:  ['advisori', 'storm', 'surg', 'damag', 'wind', 'arriv', 'soon', 'carolina', 'hurricaneian']\n",
      "=======================================13=======================================\n",
      "                                    doc_3909                                     \n",
      "\t- Score: 22.59888458251953\n",
      "\t- Text:  ['map', 'hurrican', 'ian', 'hit', 'florida', 'hardest', 'offici', 'still', 'work', 'assess', 'extent', 'damag', 'caus', 'hurricaneian', 'amp', 'subsequ', 'flood', 'report', 'emerg', 'destroy', 'home', 'damag', 'power', 'line', 'amp', 'disrupt', 'water', 'suppli', 'stormsurg']\n",
      "=======================================14=======================================\n",
      "                                    doc_3409                                     \n",
      "\t- Score: 22.542524337768555\n",
      "\t- Text:  ['well', 'check', 'hurricaneian', 'damag', 'venicefl']\n",
      "=======================================15=======================================\n",
      "                                    doc_3068                                     \n",
      "\t- Score: 22.542524337768555\n",
      "\t- Text:  ['well', 'check', 'hurricaneian', 'venicefl', 'damag']\n",
      "=======================================16=======================================\n",
      "                                    doc_1733                                     \n",
      "\t- Score: 22.499187469482422\n",
      "\t- Text:  ['three', 'day', 'landsat', 'amp', 'captur', 'imag', 'hurricaneian', 'move', 'across', 'cuba', 'amp', 'florida', 'storm', 'left', 'stagger', 'damag', 'wake', 'amp', 'million', 'peopl', 'without', 'power', 'hurrican', 'expect', 'make', 'landfal', 'south', 'carolina', 'coast', 'friday']\n",
      "=======================================17=======================================\n",
      "                                    doc_395                                      \n",
      "\t- Score: 22.48345947265625\n",
      "\t- Text:  ['storm', 'footag', 'damag', 'fort', 'myer', 'follow', 'hurrican', 'ian', 'hurricaneian', 'ianhurrican', 'hurricanian', 'hurricanefiona', 'hellojimtob', 'florida', 'miami', 'cuba', 'keywest', 'ian']\n",
      "=======================================18=======================================\n",
      "                                    doc_385                                      \n",
      "\t- Score: 22.48345947265625\n",
      "\t- Text:  ['storm', 'footag', 'damag', 'fort', 'myer', 'follow', 'hurrican', 'ian', 'hurricaneian', 'ianhurrican', 'hurricanian', 'hurricanefiona', 'hellojimtob', 'florida', 'miami', 'cuba', 'keywest', 'ian']\n",
      "=======================================19=======================================\n",
      "                                    doc_208                                      \n",
      "\t- Score: 22.48345947265625\n",
      "\t- Text:  ['storm', 'footag', 'damag', 'fort', 'myer', 'follow', 'hurrican', 'ian', 'hurricaneian', 'ianhurrican', 'hurricanian', 'hurricanefiona', 'hellojimtob', 'florida', 'miami', 'cuba', 'keywest', 'ian']\n",
      "=======================================20=======================================\n",
      "                                    doc_585                                      \n",
      "\t- Score: 22.434450149536133\n",
      "\t- Text:  ['ian', 'photo', 'flood', 'home', 'road', 'broken', 'power', 'pole', 'bloomberg', 'show', 'us', 'damag', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 20 results out of 1114 for the searched query 'Florida floodings':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2993                                     \n",
      "\t- Score: 40.15357208251953\n",
      "\t- Text:  ['hurricaneian', 'florida', 'landfal']\n",
      "=======================================2========================================\n",
      "                                    doc_3848                                     \n",
      "\t- Score: 37.880699157714844\n",
      "\t- Text:  ['care', 'florida', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_2901                                     \n",
      "\t- Score: 37.37508773803711\n",
      "\t- Text:  ['hurricaneian', 'florida', 'rain', 'flood', 'report']\n",
      "=======================================4========================================\n",
      "                                    doc_3665                                     \n",
      "\t- Score: 37.20276641845703\n",
      "\t- Text:  ['hurricaneian', 'florida', 'see', 'vote']\n",
      "=======================================5========================================\n",
      "                                    doc_470                                      \n",
      "\t- Score: 35.977149963378906\n",
      "\t- Text:  ['thank', 'god', 'hurricaneian', 'florida']\n",
      "=======================================6========================================\n",
      "                                     doc_70                                      \n",
      "\t- Score: 35.64693832397461\n",
      "\t- Text:  ['love', 'florida', 'govrondesanti', 'hurricaneian']\n",
      "=======================================7========================================\n",
      "                                    doc_2877                                     \n",
      "\t- Score: 34.11664962768555\n",
      "\t- Text:  ['hurrican', 'ian', 'cuba', 'florida', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================8========================================\n",
      "                                    doc_2360                                     \n",
      "\t- Score: 33.312381744384766\n",
      "\t- Text:  ['hurricaneian', 'fort', 'myer', 'florida', 'via']\n",
      "=======================================9========================================\n",
      "                                    doc_111                                      \n",
      "\t- Score: 32.96756362915039\n",
      "\t- Text:  ['florida', 'long', 'road', 'recoveri', 'florida', 'hurricaneian', 'hurrican', 'ian']\n",
      "=======================================10=======================================\n",
      "                                    doc_2183                                     \n",
      "\t- Score: 32.73228073120117\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================11=======================================\n",
      "                                    doc_2108                                     \n",
      "\t- Score: 32.73228073120117\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================12=======================================\n",
      "                                    doc_2051                                     \n",
      "\t- Score: 32.73228073120117\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================13=======================================\n",
      "                                    doc_1993                                     \n",
      "\t- Score: 32.73228073120117\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================14=======================================\n",
      "                                    doc_2291                                     \n",
      "\t- Score: 32.22529983520508\n",
      "\t- Text:  ['pray', 'famili', 'friend', 'florida', 'hurricaneian']\n",
      "=======================================15=======================================\n",
      "                                    doc_1672                                     \n",
      "\t- Score: 32.09315872192383\n",
      "\t- Text:  ['edgewat', 'florida', 'flood', 'hurricaneian']\n",
      "=======================================16=======================================\n",
      "                                    doc_2545                                     \n",
      "\t- Score: 31.944271087646484\n",
      "\t- Text:  ['busi', 'open', 'south', 'florida', 'hurricaneian']\n",
      "=======================================17=======================================\n",
      "                                    doc_1504                                     \n",
      "\t- Score: 31.77091407775879\n",
      "\t- Text:  ['thank', 'god', 'hurricaneian', 'florida', 'photo']\n",
      "=======================================18=======================================\n",
      "                                    doc_3766                                     \n",
      "\t- Score: 31.762134552001953\n",
      "\t- Text:  ['recoveri', 'assist', 'florida', 'busi', 'ian', 'hurricaneian']\n",
      "=======================================19=======================================\n",
      "                                    doc_823                                      \n",
      "\t- Score: 31.57929039001465\n",
      "\t- Text:  ['storm', 'surg', 'flood', 'myrtl', 'beach', 'hurricaneian']\n",
      "=======================================20=======================================\n",
      "                                    doc_1615                                     \n",
      "\t- Score: 31.44635772705078\n",
      "\t- Text:  ['pray', 'everyon', 'commun', 'florida', 'napl', 'florida', 'hurricaneian']\n",
      "================================================================================\n",
      "\n",
      "Top 20 results out of 1333 for the searched query 'Storm and wind in Florida':\n",
      "\n",
      "=======================================1========================================\n",
      "                                    doc_2993                                     \n",
      "\t- Score: 39.78998947143555\n",
      "\t- Text:  ['hurricaneian', 'florida', 'landfal']\n",
      "=======================================2========================================\n",
      "                                    doc_3848                                     \n",
      "\t- Score: 37.5085334777832\n",
      "\t- Text:  ['care', 'florida', 'hurricaneian']\n",
      "=======================================3========================================\n",
      "                                    doc_2901                                     \n",
      "\t- Score: 36.98183822631836\n",
      "\t- Text:  ['hurricaneian', 'florida', 'rain', 'flood', 'report']\n",
      "=======================================4========================================\n",
      "                                    doc_3665                                     \n",
      "\t- Score: 36.83533477783203\n",
      "\t- Text:  ['hurricaneian', 'florida', 'see', 'vote']\n",
      "=======================================5========================================\n",
      "                                    doc_470                                      \n",
      "\t- Score: 35.620845794677734\n",
      "\t- Text:  ['thank', 'god', 'hurricaneian', 'florida']\n",
      "=======================================6========================================\n",
      "                                     doc_70                                      \n",
      "\t- Score: 35.20625686645508\n",
      "\t- Text:  ['love', 'florida', 'govrondesanti', 'hurricaneian']\n",
      "=======================================7========================================\n",
      "                                    doc_2877                                     \n",
      "\t- Score: 33.814422607421875\n",
      "\t- Text:  ['hurrican', 'ian', 'cuba', 'florida', 'south', 'carolina', 'hurricaneian']\n",
      "=======================================8========================================\n",
      "                                    doc_2360                                     \n",
      "\t- Score: 32.99077224731445\n",
      "\t- Text:  ['hurricaneian', 'fort', 'myer', 'florida', 'via']\n",
      "=======================================9========================================\n",
      "                                    doc_111                                      \n",
      "\t- Score: 32.65433883666992\n",
      "\t- Text:  ['florida', 'long', 'road', 'recoveri', 'florida', 'hurricaneian', 'hurrican', 'ian']\n",
      "=======================================10=======================================\n",
      "                                    doc_3475                                     \n",
      "\t- Score: 32.58850860595703\n",
      "\t- Text:  ['yesterday', 'storm', 'hurricaneian']\n",
      "=======================================11=======================================\n",
      "                                    doc_2183                                     \n",
      "\t- Score: 32.4251708984375\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================12=======================================\n",
      "                                    doc_2108                                     \n",
      "\t- Score: 32.4251708984375\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================13=======================================\n",
      "                                    doc_2051                                     \n",
      "\t- Score: 32.4251708984375\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================14=======================================\n",
      "                                    doc_1993                                     \n",
      "\t- Score: 32.4251708984375\n",
      "\t- Text:  ['help', 'affect', 'hurrican', 'ian', 'florida', 'usatoday', 'hurricaneian']\n",
      "=======================================15=======================================\n",
      "                                    doc_2291                                     \n",
      "\t- Score: 31.91149139404297\n",
      "\t- Text:  ['pray', 'famili', 'friend', 'florida', 'hurricaneian']\n",
      "=======================================16=======================================\n",
      "                                    doc_1672                                     \n",
      "\t- Score: 31.74517250061035\n",
      "\t- Text:  ['edgewat', 'florida', 'flood', 'hurricaneian']\n",
      "=======================================17=======================================\n",
      "                                    doc_2545                                     \n",
      "\t- Score: 31.643985748291016\n",
      "\t- Text:  ['busi', 'open', 'south', 'florida', 'hurricaneian']\n",
      "=======================================18=======================================\n",
      "                                    doc_1504                                     \n",
      "\t- Score: 31.45574188232422\n",
      "\t- Text:  ['thank', 'god', 'hurricaneian', 'florida', 'photo']\n",
      "=======================================19=======================================\n",
      "                                    doc_3766                                     \n",
      "\t- Score: 31.453283309936523\n",
      "\t- Text:  ['recoveri', 'assist', 'florida', 'busi', 'ian', 'hurricaneian']\n",
      "=======================================20=======================================\n",
      "                                    doc_823                                      \n",
      "\t- Score: 31.264055252075195\n",
      "\t- Text:  ['storm', 'surg', 'flood', 'myrtl', 'beach', 'hurricaneian']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{' RANKING WITH Word2Vec + Cosine similarity ':=^80}\")\n",
    "for query in queries:\n",
    "    ranked_docs = search_word2vec(query, tweet2vec, model, new_index)\n",
    "    top = 20\n",
    "    print(f\"{'='}\" * 80)\n",
    "    print(f\"\\nTop {top} results out of {len(ranked_docs)} for the searched query '{query}':\\n\")\n",
    "    for rank, doc in enumerate(ranked_docs[:top], start=1):\n",
    "        print(f\"{rank:=^80}\")\n",
    "        print(f\"{doc[1]:^80} \\n\\t- Score: {doc[0]}\\n\\t- Text:  {data[data['Document']==doc[1]]['Clean_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Better representations?** (discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0313a563126ef0df995aedf06346400c733220b62fb353ae280fd2c670ffb391"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
