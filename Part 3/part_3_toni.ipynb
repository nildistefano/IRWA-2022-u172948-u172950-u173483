{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------- #\n",
    "# SON LOS IMPORTS DEL LAB ANTERIOR, LOS DESCOMENTAMOS A MEDIDA QUE LOS VAYAMOS USANDO #\n",
    "# ----------------------------------------------------------------------------------- #\n",
    "\n",
    "# from collections import defaultdict\n",
    "from array import array\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "# # nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "# import json\n",
    "# import random\n",
    "# import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "      <th>Clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>1575918182698979328</td>\n",
       "      <td>So this will keep spinning over us until 7 pm…...</td>\n",
       "      <td>suzjdean</td>\n",
       "      <td>Fri Sep 30 18:39:08 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/suzjdean/status/1575918182...</td>\n",
       "      <td>['keep', 'spin', 'us', 'away', 'alreadi', 'hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>1575918151862304768</td>\n",
       "      <td>Our hearts go out to all those affected by #Hu...</td>\n",
       "      <td>lytx</td>\n",
       "      <td>Fri Sep 30 18:39:01 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/lytx/status/15759181518623...</td>\n",
       "      <td>['heart', 'go', 'affect', 'hurricaneian', 'wis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document                   Id  \\\n",
       "0    doc_1  1575918182698979328   \n",
       "1    doc_2  1575918151862304768   \n",
       "\n",
       "                                                Text  Username  \\\n",
       "0  So this will keep spinning over us until 7 pm…...  suzjdean   \n",
       "1  Our hearts go out to all those affected by #Hu...      lytx   \n",
       "\n",
       "                             Date          Hashtags Mentions  Likes  Retweets  \\\n",
       "0  Fri Sep 30 18:39:08 +0000 2022  ['HurricaneIan']      NaN      0         0   \n",
       "1  Fri Sep 30 18:39:01 +0000 2022  ['HurricaneIan']      NaN      0         0   \n",
       "\n",
       "                                                 Url  \\\n",
       "0  https://twitter.com/suzjdean/status/1575918182...   \n",
       "1  https://twitter.com/lytx/status/15759181518623...   \n",
       "\n",
       "                                          Clean_text  \n",
       "0  ['keep', 'spin', 'us', 'away', 'alreadi', 'hur...  \n",
       "1  ['heart', 'go', 'affect', 'hurricaneian', 'wis...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder = '../output/'\n",
    "data_folder = '../data/'\n",
    "data = pd.read_csv(output_folder + \"lab1_tweets_df.csv\", sep='|')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line=  line.lower() ## Transform in lowercase\n",
    "    line=  line.split() ## Tokenize the text to get a list of terms\n",
    "    line= [x for x in line if x not in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line= [stemmer.stem(x) for x in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(dataframe):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    dataframe -- DataFrame containing tweet information\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    num_documents = dataframe.shape[0]\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  # term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)   # document frequencies of terms in the corpus\n",
    "    idf = defaultdict(float)\n",
    "    for row in dataframe.iterrows():\n",
    "        doc_id = row[1]['Document']\n",
    "        terms = row[1]['Clean_text']  \n",
    "        terms = eval(terms)\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[doc_id, [position]]\n",
    "                \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        # calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] = df[term]+1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 168.69 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "new_index, tf, df, idf = create_index_tfidf(data)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words analysis in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hurricaneian</td>\n",
       "      <td>3988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>florida</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>hurrican</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ian</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>help</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>amp</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>storm</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>carolina</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>flood</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>power</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>south</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>make</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>landfal</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>wind</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>damag</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>peopl</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>safe</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>get</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>impact</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>go</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word    DF\n",
       "5    hurricaneian  3988\n",
       "72        florida   881\n",
       "47       hurrican   793\n",
       "70            ian   781\n",
       "211          help   386\n",
       "122           amp   362\n",
       "25          storm   352\n",
       "87       carolina   297\n",
       "303         flood   289\n",
       "293         power   275\n",
       "86          south   260\n",
       "77           make   251\n",
       "78        landfal   245\n",
       "109          wind   238\n",
       "48          damag   231\n",
       "258         peopl   229\n",
       "15           safe   219\n",
       "388           get   216\n",
       "309        impact   214\n",
       "7              go   210"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ============================= ##\n",
    "## show top DF in the collection ##\n",
    "## ============================= ##\n",
    "\n",
    "word = []\n",
    "word_df = []\n",
    "for item in df.items():\n",
    "    word.append(item[0])\n",
    "    word_df.append(item[1])\n",
    "\n",
    "document_frequency_df = pd.DataFrame(list(zip(word,word_df)), columns=['Word', 'DF'])\n",
    "document_frequency_df.sort_values(by='DF', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based docuemnt frequency of the terms above, we define the following queries for our test cases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Floodings in South Carolina\", \n",
    "    \"HurracaineIan disaster\", \n",
    "    \"Damage of HuracaineIan\", \n",
    "    \"Florida floodings\", \n",
    "    \"Storm and wind in Florida\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New **function** `BestMatch25`:\n",
    "\n",
    "Formula: $RSV_d = \\sum_{t \\in q} log [\\frac{N}{dft}] * \\frac{(k+1)*tf_{td}}{k1*((1-b) + b*(\\frac{L_d}{L_{ave}}) ) + tfd} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will need Ld and Lavg, we will build a dictionnary with documents and their corresponding lenghts, and with it we will calculate Lave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_length = dict()\n",
    "for index, row in data.iterrows():\n",
    "    docu_length[row['Document']] = len(eval(row['Clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "lavg = 0\n",
    "for length in docu_length.values():\n",
    "    lavg += length\n",
    "lavg = round(lavg / len(docu_length))\n",
    "print(lavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_bm25 (terms, docs, index, idf, tf, k1, b, N, docu_length, lavg):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        dft = len(index[term]) #in how many documents does it appear\n",
    "        #new formula!!\n",
    "        query_vector[termIndex]=  np.log(N/dft) * (((k+1)*query_terms_count[term]) / (k1*((1-b)+b*( ld / lavg ) ) + query_terms_count[term]))\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                dft = len(index[term]) #in how many documents does it appear\n",
    "                ld = docu_length[doc] #document length\n",
    "                #new formula!!\n",
    "                doc_vectors[doc][termIndex] = np.log(N/dft) * (((k+1)*tf[term][doc_index]) / (k1*((1-b)+b*( ld / lavg ) ) + tf[term][doc_index]))\n",
    "                #doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n",
    "    \n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(doc_scores) == 0:\n",
    "        print(\"No results found for query\")\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, docu_length, lavg):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm *idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n",
    "    \n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(doc_scores) == 0:\n",
    "        print(\"No results found for query\")\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf) ##TO-DO new formula!! (new function, new attributes)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf_subset(query, index, subset):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\" and are in the subset.                       \n",
    "            term_docs=[posting[0] for posting in index[term] if posting[0] in subset]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf) ##TO-DO new formula!! (new function, new attributes)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 5 results out of 605 for the searched query 'Floodings in South Carolina':\n",
      "\n",
      "doc_254 - score 4.74475387696339\n",
      "doc_1289 - score 4.315205653041319\n",
      "doc_2874 - score 4.10872348195652\n",
      "doc_2834 - score 4.10872348195652\n",
      "doc_174 - score 4.10872348195652\n",
      "\n",
      "======================\n",
      "Top 5 results out of 131 for the searched query 'HurracaineIan disaster':\n",
      "\n",
      "doc_3964 - score 4.132642169823945\n",
      "doc_865 - score 3.792112455030452\n",
      "doc_3933 - score 3.6962351566905367\n",
      "doc_259 - score 3.6962351566905367\n",
      "doc_1635 - score 3.6962351566905367\n",
      "\n",
      "======================\n",
      "Top 5 results out of 231 for the searched query 'Damage of HuracaineIan':\n",
      "\n",
      "doc_2687 - score 2.8749627271127562\n",
      "doc_1555 - score 2.8749627271127562\n",
      "doc_1226 - score 2.638065798398665\n",
      "doc_3735 - score 2.5713666631296492\n",
      "doc_372 - score 2.5713666631296492\n",
      "\n",
      "======================\n",
      "Top 5 results out of 1114 for the searched query 'Florida floodings':\n",
      "\n",
      "doc_1317 - score 3.794380721457354\n",
      "doc_1672 - score 3.250375687450309\n",
      "doc_1493 - score 2.9694789323793103\n",
      "doc_2901 - score 2.907136014855557\n",
      "doc_824 - score 2.8189040144223148\n",
      "\n",
      "======================\n",
      "Top 5 results out of 1333 for the searched query 'Storm and wind in Florida':\n",
      "\n",
      "doc_370 - score 2.8184075410327574\n",
      "doc_2794 - score 2.77975276449882\n",
      "doc_1417 - score 2.7583100467507595\n",
      "doc_3501 - score 2.750342242635723\n",
      "doc_557 - score 2.746573006641334\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    ranked_docs = search_tf_idf(query, new_index)\n",
    "    top = 5\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for the searched query '{}':\\n\".format(top, len(ranked_docs),query))\n",
    "    for doc in ranked_docs[:top]:\n",
    "        print(\"{} - score {}\".format(doc[1], doc[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ced987a247890006fadb174404bf884c2606133e9ca5a8bdda00b49ca7b4dc36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
